{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAODWSicGa43ALnofwS0yH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungjaecheong/Binary_ANN_Programming/blob/main/CP1_DS_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Codestates AI Bootcamp 15th<br>CP1 (Codestates Project 1) - DS track\n",
        "---\n",
        "### 🍷 작성자 : AIB 15기 정경재 (Kyung Jae, Cheong)\n",
        "---"
      ],
      "metadata": {
        "id": "x9bv9cod_627"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💾 1. 라이브러리 및 데이터 불러오기"
      ],
      "metadata": {
        "id": "jzqDroLt6hkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Library Import"
      ],
      "metadata": {
        "id": "JpiDDcDi6uBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library Import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# matplotlib minus표시 설정\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "yGoJyfhs7naE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 1-1. 데이터 불러오기 : data_load()"
      ],
      "metadata": {
        "id": "-5o9mynz6xgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기 기능\n",
        "def data_load(filepath):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    filepath : csv 파일 디렉토리\n",
        "    '''\n",
        "    # open 함수로 파일을 열기 (처리가 끝나면 파일을 닫도록 with문 활용)\n",
        "    with open(file=filepath) as csv_file:\n",
        "        # csv 모듈로 파일을 읽기 (reader)\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        # 첫 열 따로 저장\n",
        "        row0 = next(csv_reader)\n",
        "        # 두번째 열부터 데이터를 list로 저장\n",
        "        data = list(csv_reader)\n",
        "    \n",
        "    # pandas로 DataFrame 생성 (column : row0)\n",
        "    dataframe = pd.DataFrame(data=data, columns=row0)\n",
        "    # 독립변수(x1~x8)는 float로, 종속변수(y)는 int로 변환\n",
        "    dataframe = dataframe.astype(float).astype({'y':int})\n",
        "    \n",
        "    # 출력 : DataFrame\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "WkoSq-qj8HIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠️ 2. 데이터 전처리 및 분리"
      ],
      "metadata": {
        "id": "JZyEDNBT60U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 2-1. 데이터 뒤섞기 기능 : data_shuffle()"
      ],
      "metadata": {
        "id": "g-2nRX-d62i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 뒤섞기 기능\n",
        "def data_shuffle(data, reset_index=True, seed=2023):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    data : 뒤섞을 데이터(DataFrame)\n",
        "    reset_index : 기본값 True, 뒤섞인 index를 재설정할지 여부\n",
        "    seed : 기본값 2023, numpy.random.seed 값\n",
        "    '''\n",
        "    # 데이터 길이를 data_length변수에 저장\n",
        "    data_length = data.shape[0]\n",
        "\n",
        "    # index 초기값 : 0 ~ n-1\n",
        "    shuffle_idx = np.arange(data_length)\n",
        "    # seed 설정 (기본값은 2023)\n",
        "    np.random.seed(seed)\n",
        "    # index array 뒤섞기\n",
        "    np.random.shuffle(shuffle_idx)\n",
        "\n",
        "    # index를 초기화하지 않을 경우\n",
        "    if reset_index==False:\n",
        "        # shuffle_idx로 reindex만 실시\n",
        "        df = data.reindex(index=shuffle_idx)\n",
        "    # index를 초기화할 경우(기본값으로 설정되어있음)\n",
        "    elif reset_index==True:\n",
        "        # shuffle_idx로 reindex 실시 + reset_index도 실시\n",
        "        df = data.reindex(index=shuffle_idx).reset_index(drop=True)\n",
        "    \n",
        "    # 출력 : 뒤섞인 dataframe\n",
        "    return df"
      ],
      "metadata": {
        "id": "l93J6dag8KuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 2-2. 데이터 표준화 기능 : standard_scaler()"
      ],
      "metadata": {
        "id": "PHtZjTdl64K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화 기능\n",
        "def standard_scaler(data, output_dim):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    data : 표준화시킬 데이터(DataFrame)\n",
        "    output_dim : 출력층의 노드 수(종속변수 수)\n",
        "    '''\n",
        "    # X와 y를 일단 분리시키도록 함\n",
        "    X = data.iloc[:,:-output_dim]\n",
        "    y = data.iloc[:,-output_dim:]\n",
        "\n",
        "    # pandas연산에 의해 column 별로 연산이 진행된다(평균값을 빼고 표준편차로 나누기)\n",
        "    std_scale_X = (X-X.mean()) / X.std()\n",
        "\n",
        "    # 표준화한 X를 y와 concat하여 다시 DataFrame을 정의\n",
        "    df = pd.concat([std_scale_X, y], axis = 1)\n",
        "\n",
        "    # 출력 표준화를 마친 dataframe\n",
        "    return df"
      ],
      "metadata": {
        "id": "korSB8AV8Lmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 2-3. 학습 및 테스트 데이터 분리 기능 : train_test_split()"
      ],
      "metadata": {
        "id": "5pXSuXkL66lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 및 테스트 데이터 분리 기능\n",
        "def train_test_split(data, train_ratio, batch_size):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    data : 분리할 데이터(DataFrame)\n",
        "    trian_ratio : 훈련 데이터 비율 ( 0 ~ 1 )\n",
        "    batch_size : mini-batch size (integer)\n",
        "    '''\n",
        "    # 데이터의 행 갯수를 data_size로 지정\n",
        "    data_size = data.shape[0]\n",
        "    # train_ratio를 곱한 값을 배치사이즈로 나누어 batch데이터의 갯수를 구함\n",
        "    batch_count = int(data_size*train_ratio) // batch_size\n",
        "    \n",
        "    # bacth데이터 갯수와 사이즈를 곱한 값이 test 데이터의 시작 인덱스가 됨\n",
        "    test_1st_idx = batch_count*batch_size\n",
        "\n",
        "    # train, test 데이터 분리(pandas)\n",
        "    train = data.iloc[:test_1st_idx,:]\n",
        "    test = data.iloc[test_1st_idx:,:]\n",
        "\n",
        "    # 출력 : train, test dataframe\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "ccDV5fzg8QD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 2-4. 독립변수 및 종속변수 분리 기능 : X_y_split()"
      ],
      "metadata": {
        "id": "PKwVQ-OF682P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 독립변수 및 종속변수 분리\n",
        "def X_y_split(data, output_dim):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    data : 분리할 데이터 (DataFrame)\n",
        "    output_dim : 출력층 노드 수 (종속변수 수)\n",
        "    '''\n",
        "    # X는 뒤에서 부터 output_dim직전까지의 column 데이터\n",
        "    X = np.array(data.iloc[:,:-output_dim])\n",
        "    # y는 뒤에서부터 output_dim이후의 column데이터 (끝에 : 를 꼭 붙여주자)\n",
        "    y = np.array(data.iloc[:,-output_dim:])\n",
        "\n",
        "    # 출력 : X, y ndarray\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "NpBsNiEx8Sfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 3. 인공신경망(ANN) 프로그래밍"
      ],
      "metadata": {
        "id": "dwNyVnTA6_CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-1. 가중치, 편향 생성 기능 : initialization_parameter()"
      ],
      "metadata": {
        "id": "xa_kDY337BDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치, 편향 생성 기능\n",
        "def initialization_parameter(input_dim, output_dim, seed=2023):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    input_dim : 입력층 노드수\n",
        "    output_dim : 출력층 노드수\n",
        "    seed : 기본값은 2023, numpy.random.seed값\n",
        "    '''\n",
        "    # seed 설정 (기본값은 2023으로 설정)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # weight ndarray생성, shape : (input_dim,output_dim)\n",
        "    weights = np.random.randn(input_dim,output_dim)\n",
        "    # bias ndarray생성, shape : (output_dim,)\n",
        "    bias = np.random.randn(output_dim)\n",
        "\n",
        "    # 출력 : weights, bias\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "kKO6xqHA8Vy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 기능 3-2. 가중치 시각화 기능 : weights_plot()"
      ],
      "metadata": {
        "id": "IIyA2wAw7CyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 시각화 기능\n",
        "def weights_plot(weights):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    weights : 가중치 (ndarray)\n",
        "    '''\n",
        "    # 가중치 ndarray -> pandas Series로 변환(인덱싱이 더 편해짐)\n",
        "    weights_series = pd.Series(np.reshape(weights,(-1)))\n",
        "    \n",
        "    # plot 크기를 6x6으로 지정\n",
        "    plt.figure(figsize=(6,6))\n",
        "    # barplot 생성\n",
        "    weights_bar = plt.bar(weights_series.index, weights_series, color = 'pink')\n",
        "    \n",
        "    # 각 bar 마다 값 text가 위쪽에 표시되도록 해보기\n",
        "    for rect in weights_bar:\n",
        "        height = rect.get_height()\n",
        "        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.3f' % height,\n",
        "                 ha='center', va='bottom', size=8)\n",
        "    # x값의 순서의 의미가 있는 건 아니라서 x축은 표시되지 않도록 설정\n",
        "    plt.gca().axes.xaxis.set_visible(False)\n",
        "\n",
        "    # barplot 출력\n",
        "    plt.title(\"Barplot of weights\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZSHfMX0n8Xf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-3. 배치데이터 얻는 함수 : get_batch_data()"
      ],
      "metadata": {
        "id": "Guvbv06V7FCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치데이터 얻는 기능\n",
        "def get_batch_data(X, y, size, n):\n",
        "    '''\n",
        "    입력값정보\n",
        "    X : 독립변수(x1~x8)\n",
        "    y : 종속변수(y)\n",
        "    size : mini-batch 크기\n",
        "    n : 0 ~ (batch수 - 1) - for 문을 통해 입력받을 값임\n",
        "    '''\n",
        "    X_batch = X[size*n : size*(n+1)]\n",
        "    y_batch = y[size*n : size*(n+1)]\n",
        "    # 출력 : 인덱스 batch크기*n부터 batch크기 만큼의 데이터\n",
        "    return X_batch, y_batch"
      ],
      "metadata": {
        "id": "SCJfVV5I8ZTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-4. 활성화함수 : sigmoid()"
      ],
      "metadata": {
        "id": "Ccc1LWyH7Gzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화함수 : sigmoid (y : logits)\n",
        "def sigmoid(y):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    y : 가중치와 편향으로 연산된 logits\n",
        "    '''\n",
        "    Y = 1 / (1 + np.exp(-y))\n",
        "    \n",
        "    # 출력값 Y : 활성화함수를 거친 최종 확률값\n",
        "    return Y"
      ],
      "metadata": {
        "id": "FIgTNyF68bOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-5. 손실함수 : sigmoid_crossentropy_logits()"
      ],
      "metadata": {
        "id": "1UafGCWG7JA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 기능 (from_logits=True, 활성화 기능과 손실함수 기능을 한번에 실시함)\n",
        "def sigmoid_crossentropy_logits(z, y):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    z : 정답 data (Label data)\n",
        "    y : 가중치와 편향으로 연산된 logits\n",
        "    '''\n",
        "    E = y*(1-z) + np.log(1 + np.exp(-y))\n",
        "    \n",
        "    # 출력값 E : 손실값\n",
        "    return E"
      ],
      "metadata": {
        "id": "0T2QWTlu8c5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-6. 손실함수 편미분함수 : sigmoid_crossentropy_logits_prime()"
      ],
      "metadata": {
        "id": "BNdxQ6p27Klf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파를 위한 손실함수 편미분함수(logit값으로 바로 편미분하여 활성화함수의 편미분 단계를 건너뜀)\n",
        "def sigmoid_crossentropy_logits_prime(z, y):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    z : 정답 data (Label data)\n",
        "    y : 가중치와 편향으로 연산된 logits\n",
        "    '''\n",
        "    dE_dy = -z + sigmoid(y)\n",
        "    \n",
        "    # 출력값 dE_dy : 손실값 E를 logit값 y로 편미분하여 얻어낸 기울기값\n",
        "    return dE_dy"
      ],
      "metadata": {
        "id": "evh3yXt28epH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 기능 3-7. 정확도 연산 기능 : accuracy_score()"
      ],
      "metadata": {
        "id": "24h_8E5d7Mqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 연산 기능\n",
        "def accuracy_score(z, y):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    z : 정답 data (Label data)\n",
        "    y : 가중치와 편향으로 연산된 logits\n",
        "    '''    \n",
        "    # pred : logit의 부호에 따라서 boolean으로 반환 (0이하는 False(0), 0초과는 True(1))\n",
        "    pred = np.greater(y, 0)\n",
        "    # real : 0과 1로 이뤄진 label data를 boolean으로 반환 (0은 False(0), 1은 True(1))\n",
        "    real = np.greater(z, 0.5)\n",
        "    \n",
        "    # correct : 예측이 label과 같은 경우 True(1)로 반환\n",
        "    correct = np.equal(pred, real)\n",
        "    \n",
        "    # accuracy : correct를 평균내면 Boolean이 int로 연산되어 정확도가 바로 구해진다\n",
        "    accuracy = np.mean(correct)\n",
        "    \n",
        "    # 출력값 : accuracy\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "U3emIGTw8gWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 인공신경망(ANN) 클래스(Class) 구현 : NeuralNetwork() - 순전파, 예측, 검증 기능"
      ],
      "metadata": {
        "id": "VKXdymtU7Oqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 순전파까지만 Class로 구현해보기(이후 추가기능들은 상속을 통해 추가할 예정)\n",
        "class NeuralNetwork:\n",
        "    # 초기함수(Class 선언시 실행되는 초기 함수)\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        input_dim : 입력층 노드 수\n",
        "        output_dim : 출력층 노드 수\n",
        "        '''\n",
        "        # initialization_parameter 함수를 실행시켜 초기 가중치와 편향을 저장\n",
        "        self.w, self.b = initialization_parameter(input_dim, output_dim)\n",
        "    \n",
        "    # 가중치 시각화 함수(가중치 변화를 확인해보기 위해 class내부에 포함시켰음)\n",
        "    def weights_plot(self):\n",
        "        # self.w로 저장되어있는 가중치를 bar plot으로 시각화함\n",
        "        weights_plot(self.w)\n",
        "    \n",
        "    # 가중합 함수 (가중치와 편향에대한 연산 결과 값(logit)을 얻어내는 기능)\n",
        "    def weight_sum(self, X):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X : 독립변수(x1~x8)\n",
        "        '''\n",
        "        # y : X와 가중치행렬(self.w)의 행렬곱 + 편향값\n",
        "        y = np.matmul(X, self.w) + self.b\n",
        "        return y\n",
        "    \n",
        "    # 순전파 기능(학습과정이 아닌 단순 순전파 기능)\n",
        "    def feed_forward(self, X_data, y_data, mb_size, verbose=1):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X_data : input 변수(x1~x8), X_train\n",
        "        y_data : label 변수(y), y_train\n",
        "        mb_size : Mini-Batch 크기\n",
        "        verbose : 기본값은 1, 훈련 결과를 출력 할지 여부를 결정함\n",
        "            verbose = 0 : 출력안함\n",
        "            verbose = 1 : 1 epoch에 대한 평균 loss와 accuracy출력\n",
        "            verbose = 2 : batch별 데이터까지 함께 출력\n",
        "        '''\n",
        "        # Batch 갯수를 계산\n",
        "        batch_count = int(X_data.shape[0]) // mb_size\n",
        "        # 손실과 정확도를 담을 빈 리스트를 생성\n",
        "        losses, accs = [], []\n",
        "        \n",
        "        # for 문으로 batch 연산을 반복함\n",
        "        for batch in range(batch_count):\n",
        "            # X(input), z(label)를 get_batch_data 함수로부터 얻어냄\n",
        "            X, z = get_batch_data(X_data, y_data, size=mb_size, n=batch)\n",
        "            # X(input) -> y(가중합) 연산\n",
        "            y = self.weight_sum(X=X)\n",
        "            # 각 값들의 손실값(E) 연산\n",
        "            E = sigmoid_crossentropy_logits(z=z, y=y)\n",
        "            # loss : 미니배치의 평균 손실값\n",
        "            loss = np.mean(E)\n",
        "            # accuracy : 미니배치의 정확도값\n",
        "            accuracy = accuracy_score(z=z, y=y)\n",
        "            \n",
        "            # 손실값과 정확도를 리스트에 append하기(소수점 셋째자리로 반올림)\n",
        "            losses.append(round(loss, 3))\n",
        "            accs.append(round(accuracy, 3))\n",
        "        \n",
        "        # verbose 기능(기본값은 1)\n",
        "        if verbose in [1,2]:\n",
        "            # 평균 loss와 평균 accuracy를 출력(소수점 셋째자리로 반올림)\n",
        "            print(\"[Epoch 1] TrainData - Loss = {:.3f}, Accuracy = {:.3f}\"\n",
        "                  .format(np.mean(losses), np.mean(accs)))\n",
        "            if verbose == 2:\n",
        "                # 각 배치별 loss와 accuracy를 확인하기 위한 출력(verbose==2로 설정해야 보임)\n",
        "                print(f\"\\tBatch Size : {mb_size}\\n\\tMini-Batches : {batch_count}\\\n",
        "                    \\n\\tLoss : {losses}\\n\\tAccuracy : {accs}\")\n",
        "        elif verbose == 0:\n",
        "            # verbose = 0 으로 주어지면 출력안하고 pass함\n",
        "            pass\n",
        "    \n",
        "    # 예측 기능(prediction, ndarray로 반환함)\n",
        "    def predict(self, X_data, threshold=0.5):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X_data : input 변수(x1~x8)\n",
        "        threshold : 기본값은 0.5, 0과 1을 구분할 임계치(기준선)\n",
        "        '''\n",
        "        # 가중합 연산\n",
        "        y = self.weight_sum(X_data)\n",
        "        # 활성화 함수를 거쳐 확률값을 얻음\n",
        "        Y = sigmoid(y)\n",
        "        # Threshold보다 작으면 0으로 예측, 크거나 같으면 1로 예측\n",
        "        pred = np.where(Y < threshold, 0, 1)\n",
        "        \n",
        "        # 출력 : 예측값(ndarray)\n",
        "        return pred\n",
        "    \n",
        "    # 검증 기능(evaluation, 테스트 데이터에 대한 순전파 결과를 바로 출력)\n",
        "    def evaluate(self, X_data, y_data):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X_data : input 변수(x1~x8), X_test\n",
        "        y_data : label 변수(y), y_test\n",
        "        '''\n",
        "        # X(input), z(label) 정의\n",
        "        X, z = X_data, y_data\n",
        "        # 가중합(logits) 연산\n",
        "        y = self.weight_sum(X_data)\n",
        "        \n",
        "        # 평균 loss값 연산\n",
        "        loss = np.mean(sigmoid_crossentropy_logits(z=z, y=y))\n",
        "        # 평균 accuracy 연산\n",
        "        accuracy = accuracy_score(z=z, y=y)\n",
        "        \n",
        "        # 검증 결과 출력하기 (소수점 셋째자리로 반올림)\n",
        "        print(\"[Evaluation] TestData - Loss = {:.3f}, Accuracy = {:.3f}\".format(loss, accuracy))"
      ],
      "metadata": {
        "id": "VkA2hINU8jNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 인공신경망(ANN) 클래스(Class) 추가구현 : NeuralNetwork_additional() - 역전파, 학습 기능을 추가 구현"
      ],
      "metadata": {
        "id": "Hwfee0tZ7Qp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 정의한 NeuralNetwork class를 상속받고 추가적으로 역전파와 학습기능을 구현하기\n",
        "class NeuralNetwork_additional(NeuralNetwork):\n",
        "    # 초기함수(Class 선언시 실행되는 초기함수 : 상속받아서 그대로 사용함)\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        input_dim : 입력층 노드 수\n",
        "        output_dim : 출력층 노드 수\n",
        "        '''\n",
        "        # super()를 통해서 self.w, self.b 변수정보를 그대로 받아와서 실행함\n",
        "        super().__init__(input_dim, output_dim)\n",
        "    \n",
        "    # 역전파 기능 (활성화 단계도 함께 계산하는 sigmoid_crossentropy_with_logits 방식 사용)\n",
        "    def feed_backward(self, X, z, y):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X : input 변수(x1~x8), X_train\n",
        "        z : label 변수(y), y_train\n",
        "        y : 가중합 연산을 통해 얻은 logits\n",
        "        '''\n",
        "        # dE/dy 연산 (sigmoid_crossentropy_logits_prime)\n",
        "        dE_dy = sigmoid_crossentropy_logits_prime(z=z, y=y)\n",
        "        # dy/dw = X의 역행렬(transpose)\n",
        "        dy_dw = X.T\n",
        "        \n",
        "        # dE/dw 가중치 기울기값 연산 (matmul을 통한 행렬곱)\n",
        "        self.dE_dw = np.matmul(dy_dw, dE_dy)\n",
        "        # dE/db 편향 기울기값 연산 (dE/dy의 합), bias와 형태 맞추기위해 axis=0으로 설정(열 방향)\n",
        "        self.dE_db = np.sum(dE_dy, axis=0) # (4x1) -> (1x1)\n",
        "    \n",
        "    # 학습 기능 (Epoch별 batch를 고려한 학습을 진행)\n",
        "    def training(self, X_data, y_data, mb_size, epochs, learning_rate=1, verbose=1):\n",
        "        '''\n",
        "        입력값 정보\n",
        "        X_data : input 변수(x1~x8), X_train\n",
        "        y_data : label 변수(y), y_train\n",
        "        mb_size : Mini-Batch 크기\n",
        "        epochs : Epoch 수\n",
        "        learning_rate : 기본값은 1로 설정함, 학습률\n",
        "        verbose : 기본값은 1, 훈련 결과를 출력 할지 여부를 결정함\n",
        "            verbose = 0 : 출력안함\n",
        "            verbose = 1 : 1 epoch에 대한 평균 loss와 accuracy출력\n",
        "            verbose = 2 : batch별 데이터까지 함께 출력\n",
        "        '''\n",
        "        # learning_rate를 lr로 저장함\n",
        "        lr = learning_rate\n",
        "        # Batch 갯수를 계산\n",
        "        batch_count = int(X_data.shape[0]) // mb_size\n",
        "        # Epoch별 손실과 정확도 리스트를 담을 history 딕셔너리를 지정\n",
        "        self.history = {'Loss': [], 'Accuracy': []}\n",
        "        \n",
        "        # 지정한 epoch 수에 따라서 반복을 진행\n",
        "        for epoch in range(epochs):\n",
        "            # 손실과 정확도를 담을 빈 리스트 지정(epoch마다 초기화됨)\n",
        "            losses, accs = [], []\n",
        "            \n",
        "            # batch 단위로 반복을 진행함\n",
        "            for batch in range(batch_count):\n",
        "                # X(input), z(label)를 get_batch_data 함수로부터 얻어냄\n",
        "                X, z = get_batch_data(X_data, y_data, size=mb_size, n=batch)\n",
        "                # X(input) -> y(가중합) 연산\n",
        "                y = self.weight_sum(X=X)\n",
        "                \n",
        "                # 각 값들의 손실값(E) 연산\n",
        "                E = sigmoid_crossentropy_logits(z=z, y=y)\n",
        "                # loss : 미니배치의 평균 손실값\n",
        "                loss = np.mean(E)\n",
        "                # accuracy : 미니배치의 정확도값\n",
        "                accuracy = accuracy_score(z=z, y=y)\n",
        "                \n",
        "                # 손실값과 정확도값을 append (소수점 셋째자리로 반올림)\n",
        "                losses.append(round(loss, 3))\n",
        "                accs.append(round(accuracy, 3))\n",
        "                \n",
        "                # 역전파 실시(self.dE_dw, self.dE_db가 업데이트 됨)\n",
        "                self.feed_backward(X=X, z=z, y=y)\n",
        "                # 가중치 업데이트 실시\n",
        "                self.w -= lr*self.dE_dw\n",
        "                # 편향 업데이트 실시\n",
        "                self.b -= lr*self.dE_db\n",
        "            \n",
        "            # epoch당 평균 loss와 평균 accuracy를 딕셔너리의 리스트에 append (소수점 셋째자리로 반올림)\n",
        "            self.history['Loss'].append(round(np.mean(losses), 3))\n",
        "            self.history['Accuracy'].append(round(np.mean(accs), 3))\n",
        "            \n",
        "            # verbose 기능(기본값은 1)\n",
        "            if verbose in [1,2]:\n",
        "                # 평균 loss와 평균 accuracy를 출력(소수점 셋째자리로 반올림)\n",
        "                print(\"\\n[Epoch {}/{}] TrainData - Loss = {:.3f}, Accuracy = {:.3f}\"\n",
        "                      .format(epoch+1, epochs, np.mean(losses), np.mean(accs)))\n",
        "                if verbose == 2:\n",
        "                    # 각 배치별 loss와 accuracy를 확인하기 위한 출력(verbose==2로 설정해야 보임)\n",
        "                    print(f\"\\tBatch Size : {mb_size}\\n\\tMini-Batchs : {batch_count}\\\n",
        "                        \\n\\tLoss : {losses}\\n\\tAccurracy : {accs}\")\n",
        "            elif verbose == 0:\n",
        "                # verbose = 0 으로 주어지면 출력안하고 pass함\n",
        "                pass"
      ],
      "metadata": {
        "id": "YUGxs-EO8l9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📈 기능3-8. 학습 곡선 시각화 기능 : plot_history()"
      ],
      "metadata": {
        "id": "uEFx1kJG7Vc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 곡선 시각화 기능\n",
        "def plot_history(NN, key, show_value=False, show_xticks=False):\n",
        "    '''\n",
        "    입력값 정보\n",
        "    NN : NeuralNetwork 인스턴스\n",
        "    key : history변수의 key값, 'Loss' or 'Accuracy'\n",
        "    show_value : 그래프에 값을 표시할지 여부 (기본값은 False로 설정)\n",
        "    show_xticks : 그래프에 x축 값을 표시할지 여부 (기본값은 False로 설정)\n",
        "    '''\n",
        "    # 그래프 y 값 : key에 해당하는 리스트\n",
        "    y = NN.history[key]\n",
        "    # 그래프 x 값 : epoch(1 ~ y길이)\n",
        "    x = np.arange(len(y))+1\n",
        "    \n",
        "    # 그래프 크기를 6x6으로 설정\n",
        "    plt.figure(figsize=(6,6))\n",
        "    # 선그래프 plot ('o-': 점표시)\n",
        "    plt.plot(y, 'o-', color='orange')\n",
        "    \n",
        "    if show_xticks == True:\n",
        "        # x축 눈금 설정\n",
        "        plt.xticks(np.arange(len(y)), labels=x)\n",
        "    elif show_xticks == False:\n",
        "        plt.xticks([])\n",
        "    \n",
        "    # y축 범위 설정\n",
        "    ylim = [0 , np.trunc(max(y)+1)]\n",
        "    plt.ylim(ylim)\n",
        "    \n",
        "    # 축 이름, 제목 설정\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(f\"{key}\")\n",
        "    plt.title(f'History of \"{key}\" in Neural Network')\n",
        "    \n",
        "    if show_value == True:\n",
        "        # 그래프에 데이터 값을 표시하기\n",
        "        for i in range(len(x)):\n",
        "            height = y[i]\n",
        "            plt.text(x[i]-1, height + ylim[1]/100, '%.3f' % height,\n",
        "                     ha='center', va='bottom', size = 8)\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3DvfC0Be8oSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💽 4. 전체 기능 동작함수 정의"
      ],
      "metadata": {
        "id": "X5S50hvn7WbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💽 기능 4-1. 전체 기능 동작 기능(순전파) : main_test()"
      ],
      "metadata": {
        "id": "fhGWOL8b7Yes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test():\n",
        "    # 1-1. 데이터 불러오기 기능\n",
        "    df = data_load(filepath=csv_dir)\n",
        "    \n",
        "    # 2-1. 데이터 뒤섞기 기능\n",
        "    df_sf = data_shuffle(df)\n",
        "    \n",
        "    # 2-2. 데이터 표준화 기능\n",
        "    df_sc = standard_scaler(df_sf, output_dim=1)\n",
        "    \n",
        "    # 2-3. 학습 및 테스트 데이터 분리 기능\n",
        "    train, test = train_test_split(df_sc, train_ratio=0.8, batch_size=4)\n",
        "    \n",
        "    # 2-4. 독립변수 및 종속변수 분리\n",
        "    X_train, y_train = X_y_split(train, output_dim=1)\n",
        "    X_test, y_test = X_y_split(test, output_dim=1)\n",
        "    \n",
        "    # 3. 인공신경망(ANN) - 순전파 (가중치 편향 생성 기능 포함)\n",
        "    NN = NeuralNetwork(input_dim=8, output_dim=1)\n",
        "    NN.feed_forward(X_train, y_train, mb_size=4, verbose=1)"
      ],
      "metadata": {
        "id": "RTkMaLN78rDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💽 기능 4-2. 전체 기능 동작 기능(학습, 시각화, 예측, 검증) : main_additional_test()\n"
      ],
      "metadata": {
        "id": "Yh-ot_wr7bfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_additional_test():\n",
        "    # 1-1. 데이터 불러오기 기능\n",
        "    df = data_load(filepath=csv_dir)\n",
        "    \n",
        "    # 2-1. 데이터 뒤섞기 기능\n",
        "    df_sf = data_shuffle(df)\n",
        "    \n",
        "    # 2-2. 데이터 표준화 기능\n",
        "    df_sc = standard_scaler(df_sf, output_dim=1)\n",
        "    \n",
        "    # 2-3. 학습 및 테스트 데이터 분리 기능\n",
        "    train, test = train_test_split(df_sc, train_ratio=0.8, batch_size=4)\n",
        "    \n",
        "    # 2-4. 독립변수 및 종속변수 분리\n",
        "    X_train, y_train = X_y_split(train, output_dim=1)\n",
        "    X_test, y_test = X_y_split(test, output_dim=1)\n",
        "    \n",
        "    # 3-1. 인공신경망(ANN) - 역전파, 학습 추가 구현\n",
        "    NN = NeuralNetwork_additional(input_dim=8, output_dim=1)\n",
        "    NN.training(X_train, y_train, mb_size=4, epochs=5, learning_rate=1, verbose=2)\n",
        "    \n",
        "    # 3-2. 학습곡선 시각화\n",
        "    plot_history(NN, key='Accuracy', show_value=True, show_xticks=True)\n",
        "    \n",
        "    # 3-3. TestData 예측\n",
        "    print(f\"[Label_data] TestData\\n{y_test}\\n\")\n",
        "    y_pred = NN.predict(X_test)\n",
        "    print(f\"[Prediction] TestData\\n{y_pred}\\n\")\n",
        "    \n",
        "    # 3-4. TestData 검증\n",
        "    NN.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "L5MoYR6e8uUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}