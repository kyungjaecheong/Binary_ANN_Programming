{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAODWSicGa43ALnofwS0yH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungjaecheong/Binary_ANN_Programming/blob/main/CP1_DS_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ Codestates AI Bootcamp 15th<br>CP1 (Codestates Project 1) - DS track\n",
        "---\n",
        "### ğŸ· ì‘ì„±ì : AIB 15ê¸° ì •ê²½ì¬ (Kyung Jae, Cheong)\n",
        "---"
      ],
      "metadata": {
        "id": "x9bv9cod_627"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ’¾ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "jzqDroLt6hkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Library Import"
      ],
      "metadata": {
        "id": "JpiDDcDi6uBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library Import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# matplotlib minusí‘œì‹œ ì„¤ì •\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "yGoJyfhs7naE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 1-1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° : data_load()"
      ],
      "metadata": {
        "id": "-5o9mynz6xgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥\n",
        "def data_load(filepath):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    filepath : csv íŒŒì¼ ë””ë ‰í† ë¦¬\n",
        "    '''\n",
        "    # open í•¨ìˆ˜ë¡œ íŒŒì¼ì„ ì—´ê¸° (ì²˜ë¦¬ê°€ ëë‚˜ë©´ íŒŒì¼ì„ ë‹«ë„ë¡ withë¬¸ í™œìš©)\n",
        "    with open(file=filepath) as csv_file:\n",
        "        # csv ëª¨ë“ˆë¡œ íŒŒì¼ì„ ì½ê¸° (reader)\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        # ì²« ì—´ ë”°ë¡œ ì €ì¥\n",
        "        row0 = next(csv_reader)\n",
        "        # ë‘ë²ˆì§¸ ì—´ë¶€í„° ë°ì´í„°ë¥¼ listë¡œ ì €ì¥\n",
        "        data = list(csv_reader)\n",
        "    \n",
        "    # pandasë¡œ DataFrame ìƒì„± (column : row0)\n",
        "    dataframe = pd.DataFrame(data=data, columns=row0)\n",
        "    # ë…ë¦½ë³€ìˆ˜(x1~x8)ëŠ” floatë¡œ, ì¢…ì†ë³€ìˆ˜(y)ëŠ” intë¡œ ë³€í™˜\n",
        "    dataframe = dataframe.astype(float).astype({'y':int})\n",
        "    \n",
        "    # ì¶œë ¥ : DataFrame\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "WkoSq-qj8HIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ› ï¸ 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ë¦¬"
      ],
      "metadata": {
        "id": "JZyEDNBT60U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 2-1. ë°ì´í„° ë’¤ì„ê¸° ê¸°ëŠ¥ : data_shuffle()"
      ],
      "metadata": {
        "id": "g-2nRX-d62i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ë’¤ì„ê¸° ê¸°ëŠ¥\n",
        "def data_shuffle(data, reset_index=True, seed=2023):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    data : ë’¤ì„ì„ ë°ì´í„°(DataFrame)\n",
        "    reset_index : ê¸°ë³¸ê°’ True, ë’¤ì„ì¸ indexë¥¼ ì¬ì„¤ì •í• ì§€ ì—¬ë¶€\n",
        "    seed : ê¸°ë³¸ê°’ 2023, numpy.random.seed ê°’\n",
        "    '''\n",
        "    # ë°ì´í„° ê¸¸ì´ë¥¼ data_lengthë³€ìˆ˜ì— ì €ì¥\n",
        "    data_length = data.shape[0]\n",
        "\n",
        "    # index ì´ˆê¸°ê°’ : 0 ~ n-1\n",
        "    shuffle_idx = np.arange(data_length)\n",
        "    # seed ì„¤ì • (ê¸°ë³¸ê°’ì€ 2023)\n",
        "    np.random.seed(seed)\n",
        "    # index array ë’¤ì„ê¸°\n",
        "    np.random.shuffle(shuffle_idx)\n",
        "\n",
        "    # indexë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•Šì„ ê²½ìš°\n",
        "    if reset_index==False:\n",
        "        # shuffle_idxë¡œ reindexë§Œ ì‹¤ì‹œ\n",
        "        df = data.reindex(index=shuffle_idx)\n",
        "    # indexë¥¼ ì´ˆê¸°í™”í•  ê²½ìš°(ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ìˆìŒ)\n",
        "    elif reset_index==True:\n",
        "        # shuffle_idxë¡œ reindex ì‹¤ì‹œ + reset_indexë„ ì‹¤ì‹œ\n",
        "        df = data.reindex(index=shuffle_idx).reset_index(drop=True)\n",
        "    \n",
        "    # ì¶œë ¥ : ë’¤ì„ì¸ dataframe\n",
        "    return df"
      ],
      "metadata": {
        "id": "l93J6dag8KuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 2-2. ë°ì´í„° í‘œì¤€í™” ê¸°ëŠ¥ : standard_scaler()"
      ],
      "metadata": {
        "id": "PHtZjTdl64K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° í‘œì¤€í™” ê¸°ëŠ¥\n",
        "def standard_scaler(data, output_dim):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    data : í‘œì¤€í™”ì‹œí‚¬ ë°ì´í„°(DataFrame)\n",
        "    output_dim : ì¶œë ¥ì¸µì˜ ë…¸ë“œ ìˆ˜(ì¢…ì†ë³€ìˆ˜ ìˆ˜)\n",
        "    '''\n",
        "    # Xì™€ yë¥¼ ì¼ë‹¨ ë¶„ë¦¬ì‹œí‚¤ë„ë¡ í•¨\n",
        "    X = data.iloc[:,:-output_dim]\n",
        "    y = data.iloc[:,-output_dim:]\n",
        "\n",
        "    # pandasì—°ì‚°ì— ì˜í•´ column ë³„ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤(í‰ê· ê°’ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ê¸°)\n",
        "    std_scale_X = (X-X.mean()) / X.std()\n",
        "\n",
        "    # í‘œì¤€í™”í•œ Xë¥¼ yì™€ concatí•˜ì—¬ ë‹¤ì‹œ DataFrameì„ ì •ì˜\n",
        "    df = pd.concat([std_scale_X, y], axis = 1)\n",
        "\n",
        "    # ì¶œë ¥ í‘œì¤€í™”ë¥¼ ë§ˆì¹œ dataframe\n",
        "    return df"
      ],
      "metadata": {
        "id": "korSB8AV8Lmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 2-3. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ê¸°ëŠ¥ : train_test_split()"
      ],
      "metadata": {
        "id": "5pXSuXkL66lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ê¸°ëŠ¥\n",
        "def train_test_split(data, train_ratio, batch_size):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    data : ë¶„ë¦¬í•  ë°ì´í„°(DataFrame)\n",
        "    trian_ratio : í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ ( 0 ~ 1 )\n",
        "    batch_size : mini-batch size (integer)\n",
        "    '''\n",
        "    # ë°ì´í„°ì˜ í–‰ ê°¯ìˆ˜ë¥¼ data_sizeë¡œ ì§€ì •\n",
        "    data_size = data.shape[0]\n",
        "    # train_ratioë¥¼ ê³±í•œ ê°’ì„ ë°°ì¹˜ì‚¬ì´ì¦ˆë¡œ ë‚˜ëˆ„ì–´ batchë°ì´í„°ì˜ ê°¯ìˆ˜ë¥¼ êµ¬í•¨\n",
        "    batch_count = int(data_size*train_ratio) // batch_size\n",
        "    \n",
        "    # bacthë°ì´í„° ê°¯ìˆ˜ì™€ ì‚¬ì´ì¦ˆë¥¼ ê³±í•œ ê°’ì´ test ë°ì´í„°ì˜ ì‹œì‘ ì¸ë±ìŠ¤ê°€ ë¨\n",
        "    test_1st_idx = batch_count*batch_size\n",
        "\n",
        "    # train, test ë°ì´í„° ë¶„ë¦¬(pandas)\n",
        "    train = data.iloc[:test_1st_idx,:]\n",
        "    test = data.iloc[test_1st_idx:,:]\n",
        "\n",
        "    # ì¶œë ¥ : train, test dataframe\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "ccDV5fzg8QD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 2-4. ë…ë¦½ë³€ìˆ˜ ë° ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬ ê¸°ëŠ¥ : X_y_split()"
      ],
      "metadata": {
        "id": "PKwVQ-OF682P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë…ë¦½ë³€ìˆ˜ ë° ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬\n",
        "def X_y_split(data, output_dim):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    data : ë¶„ë¦¬í•  ë°ì´í„° (DataFrame)\n",
        "    output_dim : ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜ (ì¢…ì†ë³€ìˆ˜ ìˆ˜)\n",
        "    '''\n",
        "    # XëŠ” ë’¤ì—ì„œ ë¶€í„° output_dimì§ì „ê¹Œì§€ì˜ column ë°ì´í„°\n",
        "    X = np.array(data.iloc[:,:-output_dim])\n",
        "    # yëŠ” ë’¤ì—ì„œë¶€í„° output_dimì´í›„ì˜ columnë°ì´í„° (ëì— : ë¥¼ ê¼­ ë¶™ì—¬ì£¼ì)\n",
        "    y = np.array(data.iloc[:,-output_dim:])\n",
        "\n",
        "    # ì¶œë ¥ : X, y ndarray\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "NpBsNiEx8Sfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  3. ì¸ê³µì‹ ê²½ë§(ANN) í”„ë¡œê·¸ë˜ë°"
      ],
      "metadata": {
        "id": "dwNyVnTA6_CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-1. ê°€ì¤‘ì¹˜, í¸í–¥ ìƒì„± ê¸°ëŠ¥ : initialization_parameter()"
      ],
      "metadata": {
        "id": "xa_kDY337BDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°€ì¤‘ì¹˜, í¸í–¥ ìƒì„± ê¸°ëŠ¥\n",
        "def initialization_parameter(input_dim, output_dim, seed=2023):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    input_dim : ì…ë ¥ì¸µ ë…¸ë“œìˆ˜\n",
        "    output_dim : ì¶œë ¥ì¸µ ë…¸ë“œìˆ˜\n",
        "    seed : ê¸°ë³¸ê°’ì€ 2023, numpy.random.seedê°’\n",
        "    '''\n",
        "    # seed ì„¤ì • (ê¸°ë³¸ê°’ì€ 2023ìœ¼ë¡œ ì„¤ì •)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # weight ndarrayìƒì„±, shape : (input_dim,output_dim)\n",
        "    weights = np.random.randn(input_dim,output_dim)\n",
        "    # bias ndarrayìƒì„±, shape : (output_dim,)\n",
        "    bias = np.random.randn(output_dim)\n",
        "\n",
        "    # ì¶œë ¥ : weights, bias\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "kKO6xqHA8Vy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š ê¸°ëŠ¥ 3-2. ê°€ì¤‘ì¹˜ ì‹œê°í™” ê¸°ëŠ¥ : weights_plot()"
      ],
      "metadata": {
        "id": "IIyA2wAw7CyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°€ì¤‘ì¹˜ ì‹œê°í™” ê¸°ëŠ¥\n",
        "def weights_plot(weights):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    weights : ê°€ì¤‘ì¹˜ (ndarray)\n",
        "    '''\n",
        "    # ê°€ì¤‘ì¹˜ ndarray -> pandas Seriesë¡œ ë³€í™˜(ì¸ë±ì‹±ì´ ë” í¸í•´ì§)\n",
        "    weights_series = pd.Series(np.reshape(weights,(-1)))\n",
        "    \n",
        "    # plot í¬ê¸°ë¥¼ 6x6ìœ¼ë¡œ ì§€ì •\n",
        "    plt.figure(figsize=(6,6))\n",
        "    # barplot ìƒì„±\n",
        "    weights_bar = plt.bar(weights_series.index, weights_series, color = 'pink')\n",
        "    \n",
        "    # ê° bar ë§ˆë‹¤ ê°’ textê°€ ìœ„ìª½ì— í‘œì‹œë˜ë„ë¡ í•´ë³´ê¸°\n",
        "    for rect in weights_bar:\n",
        "        height = rect.get_height()\n",
        "        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.3f' % height,\n",
        "                 ha='center', va='bottom', size=8)\n",
        "    # xê°’ì˜ ìˆœì„œì˜ ì˜ë¯¸ê°€ ìˆëŠ” ê±´ ì•„ë‹ˆë¼ì„œ xì¶•ì€ í‘œì‹œë˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
        "    plt.gca().axes.xaxis.set_visible(False)\n",
        "\n",
        "    # barplot ì¶œë ¥\n",
        "    plt.title(\"Barplot of weights\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZSHfMX0n8Xf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-3. ë°°ì¹˜ë°ì´í„° ì–»ëŠ” í•¨ìˆ˜ : get_batch_data()"
      ],
      "metadata": {
        "id": "Guvbv06V7FCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°°ì¹˜ë°ì´í„° ì–»ëŠ” ê¸°ëŠ¥\n",
        "def get_batch_data(X, y, size, n):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ì •ë³´\n",
        "    X : ë…ë¦½ë³€ìˆ˜(x1~x8)\n",
        "    y : ì¢…ì†ë³€ìˆ˜(y)\n",
        "    size : mini-batch í¬ê¸°\n",
        "    n : 0 ~ (batchìˆ˜ - 1) - for ë¬¸ì„ í†µí•´ ì…ë ¥ë°›ì„ ê°’ì„\n",
        "    '''\n",
        "    X_batch = X[size*n : size*(n+1)]\n",
        "    y_batch = y[size*n : size*(n+1)]\n",
        "    # ì¶œë ¥ : ì¸ë±ìŠ¤ batchí¬ê¸°*në¶€í„° batchí¬ê¸° ë§Œí¼ì˜ ë°ì´í„°\n",
        "    return X_batch, y_batch"
      ],
      "metadata": {
        "id": "SCJfVV5I8ZTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-4. í™œì„±í™”í•¨ìˆ˜ : sigmoid()"
      ],
      "metadata": {
        "id": "Ccc1LWyH7Gzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í™œì„±í™”í•¨ìˆ˜ : sigmoid (y : logits)\n",
        "def sigmoid(y):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    y : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ìœ¼ë¡œ ì—°ì‚°ëœ logits\n",
        "    '''\n",
        "    Y = 1 / (1 + np.exp(-y))\n",
        "    \n",
        "    # ì¶œë ¥ê°’ Y : í™œì„±í™”í•¨ìˆ˜ë¥¼ ê±°ì¹œ ìµœì¢… í™•ë¥ ê°’\n",
        "    return Y"
      ],
      "metadata": {
        "id": "FIgTNyF68bOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-5. ì†ì‹¤í•¨ìˆ˜ : sigmoid_crossentropy_logits()"
      ],
      "metadata": {
        "id": "1UafGCWG7JA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì†ì‹¤ í•¨ìˆ˜ ê¸°ëŠ¥ (from_logits=True, í™œì„±í™” ê¸°ëŠ¥ê³¼ ì†ì‹¤í•¨ìˆ˜ ê¸°ëŠ¥ì„ í•œë²ˆì— ì‹¤ì‹œí•¨)\n",
        "def sigmoid_crossentropy_logits(z, y):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    z : ì •ë‹µ data (Label data)\n",
        "    y : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ìœ¼ë¡œ ì—°ì‚°ëœ logits\n",
        "    '''\n",
        "    E = y*(1-z) + np.log(1 + np.exp(-y))\n",
        "    \n",
        "    # ì¶œë ¥ê°’ E : ì†ì‹¤ê°’\n",
        "    return E"
      ],
      "metadata": {
        "id": "0T2QWTlu8c5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-6. ì†ì‹¤í•¨ìˆ˜ í¸ë¯¸ë¶„í•¨ìˆ˜ : sigmoid_crossentropy_logits_prime()"
      ],
      "metadata": {
        "id": "BNdxQ6p27Klf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—­ì „íŒŒë¥¼ ìœ„í•œ ì†ì‹¤í•¨ìˆ˜ í¸ë¯¸ë¶„í•¨ìˆ˜(logitê°’ìœ¼ë¡œ ë°”ë¡œ í¸ë¯¸ë¶„í•˜ì—¬ í™œì„±í™”í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœ€)\n",
        "def sigmoid_crossentropy_logits_prime(z, y):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    z : ì •ë‹µ data (Label data)\n",
        "    y : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ìœ¼ë¡œ ì—°ì‚°ëœ logits\n",
        "    '''\n",
        "    dE_dy = -z + sigmoid(y)\n",
        "    \n",
        "    # ì¶œë ¥ê°’ dE_dy : ì†ì‹¤ê°’ Eë¥¼ logitê°’ yë¡œ í¸ë¯¸ë¶„í•˜ì—¬ ì–»ì–´ë‚¸ ê¸°ìš¸ê¸°ê°’\n",
        "    return dE_dy"
      ],
      "metadata": {
        "id": "evh3yXt28epH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ ê¸°ëŠ¥ 3-7. ì •í™•ë„ ì—°ì‚° ê¸°ëŠ¥ : accuracy_score()"
      ],
      "metadata": {
        "id": "24h_8E5d7Mqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì •í™•ë„ ì—°ì‚° ê¸°ëŠ¥\n",
        "def accuracy_score(z, y):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    z : ì •ë‹µ data (Label data)\n",
        "    y : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ìœ¼ë¡œ ì—°ì‚°ëœ logits\n",
        "    '''    \n",
        "    # pred : logitì˜ ë¶€í˜¸ì— ë”°ë¼ì„œ booleanìœ¼ë¡œ ë°˜í™˜ (0ì´í•˜ëŠ” False(0), 0ì´ˆê³¼ëŠ” True(1))\n",
        "    pred = np.greater(y, 0)\n",
        "    # real : 0ê³¼ 1ë¡œ ì´ë¤„ì§„ label dataë¥¼ booleanìœ¼ë¡œ ë°˜í™˜ (0ì€ False(0), 1ì€ True(1))\n",
        "    real = np.greater(z, 0.5)\n",
        "    \n",
        "    # correct : ì˜ˆì¸¡ì´ labelê³¼ ê°™ì€ ê²½ìš° True(1)ë¡œ ë°˜í™˜\n",
        "    correct = np.equal(pred, real)\n",
        "    \n",
        "    # accuracy : correctë¥¼ í‰ê· ë‚´ë©´ Booleanì´ intë¡œ ì—°ì‚°ë˜ì–´ ì •í™•ë„ê°€ ë°”ë¡œ êµ¬í•´ì§„ë‹¤\n",
        "    accuracy = np.mean(correct)\n",
        "    \n",
        "    # ì¶œë ¥ê°’ : accuracy\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "U3emIGTw8gWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  ì¸ê³µì‹ ê²½ë§(ANN) í´ë˜ìŠ¤(Class) êµ¬í˜„ : NeuralNetwork() - ìˆœì „íŒŒ, ì˜ˆì¸¡, ê²€ì¦ ê¸°ëŠ¥"
      ],
      "metadata": {
        "id": "VKXdymtU7Oqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìˆœì „íŒŒê¹Œì§€ë§Œ Classë¡œ êµ¬í˜„í•´ë³´ê¸°(ì´í›„ ì¶”ê°€ê¸°ëŠ¥ë“¤ì€ ìƒì†ì„ í†µí•´ ì¶”ê°€í•  ì˜ˆì •)\n",
        "class NeuralNetwork:\n",
        "    # ì´ˆê¸°í•¨ìˆ˜(Class ì„ ì–¸ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ˆê¸° í•¨ìˆ˜)\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        input_dim : ì…ë ¥ì¸µ ë…¸ë“œ ìˆ˜\n",
        "        output_dim : ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜\n",
        "        '''\n",
        "        # initialization_parameter í•¨ìˆ˜ë¥¼ ì‹¤í–‰ì‹œì¼œ ì´ˆê¸° ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì €ì¥\n",
        "        self.w, self.b = initialization_parameter(input_dim, output_dim)\n",
        "    \n",
        "    # ê°€ì¤‘ì¹˜ ì‹œê°í™” í•¨ìˆ˜(ê°€ì¤‘ì¹˜ ë³€í™”ë¥¼ í™•ì¸í•´ë³´ê¸° ìœ„í•´ classë‚´ë¶€ì— í¬í•¨ì‹œì¼°ìŒ)\n",
        "    def weights_plot(self):\n",
        "        # self.wë¡œ ì €ì¥ë˜ì–´ìˆëŠ” ê°€ì¤‘ì¹˜ë¥¼ bar plotìœ¼ë¡œ ì‹œê°í™”í•¨\n",
        "        weights_plot(self.w)\n",
        "    \n",
        "    # ê°€ì¤‘í•© í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì—ëŒ€í•œ ì—°ì‚° ê²°ê³¼ ê°’(logit)ì„ ì–»ì–´ë‚´ëŠ” ê¸°ëŠ¥)\n",
        "    def weight_sum(self, X):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X : ë…ë¦½ë³€ìˆ˜(x1~x8)\n",
        "        '''\n",
        "        # y : Xì™€ ê°€ì¤‘ì¹˜í–‰ë ¬(self.w)ì˜ í–‰ë ¬ê³± + í¸í–¥ê°’\n",
        "        y = np.matmul(X, self.w) + self.b\n",
        "        return y\n",
        "    \n",
        "    # ìˆœì „íŒŒ ê¸°ëŠ¥(í•™ìŠµê³¼ì •ì´ ì•„ë‹Œ ë‹¨ìˆœ ìˆœì „íŒŒ ê¸°ëŠ¥)\n",
        "    def feed_forward(self, X_data, y_data, mb_size, verbose=1):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X_data : input ë³€ìˆ˜(x1~x8), X_train\n",
        "        y_data : label ë³€ìˆ˜(y), y_train\n",
        "        mb_size : Mini-Batch í¬ê¸°\n",
        "        verbose : ê¸°ë³¸ê°’ì€ 1, í›ˆë ¨ ê²°ê³¼ë¥¼ ì¶œë ¥ í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•¨\n",
        "            verbose = 0 : ì¶œë ¥ì•ˆí•¨\n",
        "            verbose = 1 : 1 epochì— ëŒ€í•œ í‰ê·  lossì™€ accuracyì¶œë ¥\n",
        "            verbose = 2 : batchë³„ ë°ì´í„°ê¹Œì§€ í•¨ê»˜ ì¶œë ¥\n",
        "        '''\n",
        "        # Batch ê°¯ìˆ˜ë¥¼ ê³„ì‚°\n",
        "        batch_count = int(X_data.shape[0]) // mb_size\n",
        "        # ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ë‹´ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±\n",
        "        losses, accs = [], []\n",
        "        \n",
        "        # for ë¬¸ìœ¼ë¡œ batch ì—°ì‚°ì„ ë°˜ë³µí•¨\n",
        "        for batch in range(batch_count):\n",
        "            # X(input), z(label)ë¥¼ get_batch_data í•¨ìˆ˜ë¡œë¶€í„° ì–»ì–´ëƒ„\n",
        "            X, z = get_batch_data(X_data, y_data, size=mb_size, n=batch)\n",
        "            # X(input) -> y(ê°€ì¤‘í•©) ì—°ì‚°\n",
        "            y = self.weight_sum(X=X)\n",
        "            # ê° ê°’ë“¤ì˜ ì†ì‹¤ê°’(E) ì—°ì‚°\n",
        "            E = sigmoid_crossentropy_logits(z=z, y=y)\n",
        "            # loss : ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê·  ì†ì‹¤ê°’\n",
        "            loss = np.mean(E)\n",
        "            # accuracy : ë¯¸ë‹ˆë°°ì¹˜ì˜ ì •í™•ë„ê°’\n",
        "            accuracy = accuracy_score(z=z, y=y)\n",
        "            \n",
        "            # ì†ì‹¤ê°’ê³¼ ì •í™•ë„ë¥¼ ë¦¬ìŠ¤íŠ¸ì— appendí•˜ê¸°(ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "            losses.append(round(loss, 3))\n",
        "            accs.append(round(accuracy, 3))\n",
        "        \n",
        "        # verbose ê¸°ëŠ¥(ê¸°ë³¸ê°’ì€ 1)\n",
        "        if verbose in [1,2]:\n",
        "            # í‰ê·  lossì™€ í‰ê·  accuracyë¥¼ ì¶œë ¥(ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "            print(\"[Epoch 1] TrainData - Loss = {:.3f}, Accuracy = {:.3f}\"\n",
        "                  .format(np.mean(losses), np.mean(accs)))\n",
        "            if verbose == 2:\n",
        "                # ê° ë°°ì¹˜ë³„ lossì™€ accuracyë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì¶œë ¥(verbose==2ë¡œ ì„¤ì •í•´ì•¼ ë³´ì„)\n",
        "                print(f\"\\tBatch Size : {mb_size}\\n\\tMini-Batches : {batch_count}\\\n",
        "                    \\n\\tLoss : {losses}\\n\\tAccuracy : {accs}\")\n",
        "        elif verbose == 0:\n",
        "            # verbose = 0 ìœ¼ë¡œ ì£¼ì–´ì§€ë©´ ì¶œë ¥ì•ˆí•˜ê³  passí•¨\n",
        "            pass\n",
        "    \n",
        "    # ì˜ˆì¸¡ ê¸°ëŠ¥(prediction, ndarrayë¡œ ë°˜í™˜í•¨)\n",
        "    def predict(self, X_data, threshold=0.5):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X_data : input ë³€ìˆ˜(x1~x8)\n",
        "        threshold : ê¸°ë³¸ê°’ì€ 0.5, 0ê³¼ 1ì„ êµ¬ë¶„í•  ì„ê³„ì¹˜(ê¸°ì¤€ì„ )\n",
        "        '''\n",
        "        # ê°€ì¤‘í•© ì—°ì‚°\n",
        "        y = self.weight_sum(X_data)\n",
        "        # í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì³ í™•ë¥ ê°’ì„ ì–»ìŒ\n",
        "        Y = sigmoid(y)\n",
        "        # Thresholdë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ì˜ˆì¸¡, í¬ê±°ë‚˜ ê°™ìœ¼ë©´ 1ë¡œ ì˜ˆì¸¡\n",
        "        pred = np.where(Y < threshold, 0, 1)\n",
        "        \n",
        "        # ì¶œë ¥ : ì˜ˆì¸¡ê°’(ndarray)\n",
        "        return pred\n",
        "    \n",
        "    # ê²€ì¦ ê¸°ëŠ¥(evaluation, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ìˆœì „íŒŒ ê²°ê³¼ë¥¼ ë°”ë¡œ ì¶œë ¥)\n",
        "    def evaluate(self, X_data, y_data):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X_data : input ë³€ìˆ˜(x1~x8), X_test\n",
        "        y_data : label ë³€ìˆ˜(y), y_test\n",
        "        '''\n",
        "        # X(input), z(label) ì •ì˜\n",
        "        X, z = X_data, y_data\n",
        "        # ê°€ì¤‘í•©(logits) ì—°ì‚°\n",
        "        y = self.weight_sum(X_data)\n",
        "        \n",
        "        # í‰ê·  lossê°’ ì—°ì‚°\n",
        "        loss = np.mean(sigmoid_crossentropy_logits(z=z, y=y))\n",
        "        # í‰ê·  accuracy ì—°ì‚°\n",
        "        accuracy = accuracy_score(z=z, y=y)\n",
        "        \n",
        "        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥í•˜ê¸° (ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "        print(\"[Evaluation] TestData - Loss = {:.3f}, Accuracy = {:.3f}\".format(loss, accuracy))"
      ],
      "metadata": {
        "id": "VkA2hINU8jNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  ì¸ê³µì‹ ê²½ë§(ANN) í´ë˜ìŠ¤(Class) ì¶”ê°€êµ¬í˜„ : NeuralNetwork_additional() - ì—­ì „íŒŒ, í•™ìŠµ ê¸°ëŠ¥ì„ ì¶”ê°€ êµ¬í˜„"
      ],
      "metadata": {
        "id": "Hwfee0tZ7Qp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ì „ì— ì •ì˜í•œ NeuralNetwork classë¥¼ ìƒì†ë°›ê³  ì¶”ê°€ì ìœ¼ë¡œ ì—­ì „íŒŒì™€ í•™ìŠµê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê¸°\n",
        "class NeuralNetwork_additional(NeuralNetwork):\n",
        "    # ì´ˆê¸°í•¨ìˆ˜(Class ì„ ì–¸ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ˆê¸°í•¨ìˆ˜ : ìƒì†ë°›ì•„ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•¨)\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        input_dim : ì…ë ¥ì¸µ ë…¸ë“œ ìˆ˜\n",
        "        output_dim : ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜\n",
        "        '''\n",
        "        # super()ë¥¼ í†µí•´ì„œ self.w, self.b ë³€ìˆ˜ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„ì™€ì„œ ì‹¤í–‰í•¨\n",
        "        super().__init__(input_dim, output_dim)\n",
        "    \n",
        "    # ì—­ì „íŒŒ ê¸°ëŠ¥ (í™œì„±í™” ë‹¨ê³„ë„ í•¨ê»˜ ê³„ì‚°í•˜ëŠ” sigmoid_crossentropy_with_logits ë°©ì‹ ì‚¬ìš©)\n",
        "    def feed_backward(self, X, z, y):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X : input ë³€ìˆ˜(x1~x8), X_train\n",
        "        z : label ë³€ìˆ˜(y), y_train\n",
        "        y : ê°€ì¤‘í•© ì—°ì‚°ì„ í†µí•´ ì–»ì€ logits\n",
        "        '''\n",
        "        # dE/dy ì—°ì‚° (sigmoid_crossentropy_logits_prime)\n",
        "        dE_dy = sigmoid_crossentropy_logits_prime(z=z, y=y)\n",
        "        # dy/dw = Xì˜ ì—­í–‰ë ¬(transpose)\n",
        "        dy_dw = X.T\n",
        "        \n",
        "        # dE/dw ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸°ê°’ ì—°ì‚° (matmulì„ í†µí•œ í–‰ë ¬ê³±)\n",
        "        self.dE_dw = np.matmul(dy_dw, dE_dy)\n",
        "        # dE/db í¸í–¥ ê¸°ìš¸ê¸°ê°’ ì—°ì‚° (dE/dyì˜ í•©), biasì™€ í˜•íƒœ ë§ì¶”ê¸°ìœ„í•´ axis=0ìœ¼ë¡œ ì„¤ì •(ì—´ ë°©í–¥)\n",
        "        self.dE_db = np.sum(dE_dy, axis=0) # (4x1) -> (1x1)\n",
        "    \n",
        "    # í•™ìŠµ ê¸°ëŠ¥ (Epochë³„ batchë¥¼ ê³ ë ¤í•œ í•™ìŠµì„ ì§„í–‰)\n",
        "    def training(self, X_data, y_data, mb_size, epochs, learning_rate=1, verbose=1):\n",
        "        '''\n",
        "        ì…ë ¥ê°’ ì •ë³´\n",
        "        X_data : input ë³€ìˆ˜(x1~x8), X_train\n",
        "        y_data : label ë³€ìˆ˜(y), y_train\n",
        "        mb_size : Mini-Batch í¬ê¸°\n",
        "        epochs : Epoch ìˆ˜\n",
        "        learning_rate : ê¸°ë³¸ê°’ì€ 1ë¡œ ì„¤ì •í•¨, í•™ìŠµë¥ \n",
        "        verbose : ê¸°ë³¸ê°’ì€ 1, í›ˆë ¨ ê²°ê³¼ë¥¼ ì¶œë ¥ í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•¨\n",
        "            verbose = 0 : ì¶œë ¥ì•ˆí•¨\n",
        "            verbose = 1 : 1 epochì— ëŒ€í•œ í‰ê·  lossì™€ accuracyì¶œë ¥\n",
        "            verbose = 2 : batchë³„ ë°ì´í„°ê¹Œì§€ í•¨ê»˜ ì¶œë ¥\n",
        "        '''\n",
        "        # learning_rateë¥¼ lrë¡œ ì €ì¥í•¨\n",
        "        lr = learning_rate\n",
        "        # Batch ê°¯ìˆ˜ë¥¼ ê³„ì‚°\n",
        "        batch_count = int(X_data.shape[0]) // mb_size\n",
        "        # Epochë³„ ì†ì‹¤ê³¼ ì •í™•ë„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹´ì„ history ë”•ì…”ë„ˆë¦¬ë¥¼ ì§€ì •\n",
        "        self.history = {'Loss': [], 'Accuracy': []}\n",
        "        \n",
        "        # ì§€ì •í•œ epoch ìˆ˜ì— ë”°ë¼ì„œ ë°˜ë³µì„ ì§„í–‰\n",
        "        for epoch in range(epochs):\n",
        "            # ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ë‹´ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì§€ì •(epochë§ˆë‹¤ ì´ˆê¸°í™”ë¨)\n",
        "            losses, accs = [], []\n",
        "            \n",
        "            # batch ë‹¨ìœ„ë¡œ ë°˜ë³µì„ ì§„í–‰í•¨\n",
        "            for batch in range(batch_count):\n",
        "                # X(input), z(label)ë¥¼ get_batch_data í•¨ìˆ˜ë¡œë¶€í„° ì–»ì–´ëƒ„\n",
        "                X, z = get_batch_data(X_data, y_data, size=mb_size, n=batch)\n",
        "                # X(input) -> y(ê°€ì¤‘í•©) ì—°ì‚°\n",
        "                y = self.weight_sum(X=X)\n",
        "                \n",
        "                # ê° ê°’ë“¤ì˜ ì†ì‹¤ê°’(E) ì—°ì‚°\n",
        "                E = sigmoid_crossentropy_logits(z=z, y=y)\n",
        "                # loss : ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê·  ì†ì‹¤ê°’\n",
        "                loss = np.mean(E)\n",
        "                # accuracy : ë¯¸ë‹ˆë°°ì¹˜ì˜ ì •í™•ë„ê°’\n",
        "                accuracy = accuracy_score(z=z, y=y)\n",
        "                \n",
        "                # ì†ì‹¤ê°’ê³¼ ì •í™•ë„ê°’ì„ append (ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "                losses.append(round(loss, 3))\n",
        "                accs.append(round(accuracy, 3))\n",
        "                \n",
        "                # ì—­ì „íŒŒ ì‹¤ì‹œ(self.dE_dw, self.dE_dbê°€ ì—…ë°ì´íŠ¸ ë¨)\n",
        "                self.feed_backward(X=X, z=z, y=y)\n",
        "                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤ì‹œ\n",
        "                self.w -= lr*self.dE_dw\n",
        "                # í¸í–¥ ì—…ë°ì´íŠ¸ ì‹¤ì‹œ\n",
        "                self.b -= lr*self.dE_db\n",
        "            \n",
        "            # epochë‹¹ í‰ê·  lossì™€ í‰ê·  accuracyë¥¼ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸ì— append (ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "            self.history['Loss'].append(round(np.mean(losses), 3))\n",
        "            self.history['Accuracy'].append(round(np.mean(accs), 3))\n",
        "            \n",
        "            # verbose ê¸°ëŠ¥(ê¸°ë³¸ê°’ì€ 1)\n",
        "            if verbose in [1,2]:\n",
        "                # í‰ê·  lossì™€ í‰ê·  accuracyë¥¼ ì¶œë ¥(ì†Œìˆ˜ì  ì…‹ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼)\n",
        "                print(\"\\n[Epoch {}/{}] TrainData - Loss = {:.3f}, Accuracy = {:.3f}\"\n",
        "                      .format(epoch+1, epochs, np.mean(losses), np.mean(accs)))\n",
        "                if verbose == 2:\n",
        "                    # ê° ë°°ì¹˜ë³„ lossì™€ accuracyë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì¶œë ¥(verbose==2ë¡œ ì„¤ì •í•´ì•¼ ë³´ì„)\n",
        "                    print(f\"\\tBatch Size : {mb_size}\\n\\tMini-Batchs : {batch_count}\\\n",
        "                        \\n\\tLoss : {losses}\\n\\tAccurracy : {accs}\")\n",
        "            elif verbose == 0:\n",
        "                # verbose = 0 ìœ¼ë¡œ ì£¼ì–´ì§€ë©´ ì¶œë ¥ì•ˆí•˜ê³  passí•¨\n",
        "                pass"
      ],
      "metadata": {
        "id": "YUGxs-EO8l9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“ˆ ê¸°ëŠ¥3-8. í•™ìŠµ ê³¡ì„  ì‹œê°í™” ê¸°ëŠ¥ : plot_history()"
      ],
      "metadata": {
        "id": "uEFx1kJG7Vc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„  ì‹œê°í™” ê¸°ëŠ¥\n",
        "def plot_history(NN, key, show_value=False, show_xticks=False):\n",
        "    '''\n",
        "    ì…ë ¥ê°’ ì •ë³´\n",
        "    NN : NeuralNetwork ì¸ìŠ¤í„´ìŠ¤\n",
        "    key : historyë³€ìˆ˜ì˜ keyê°’, 'Loss' or 'Accuracy'\n",
        "    show_value : ê·¸ë˜í”„ì— ê°’ì„ í‘œì‹œí• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ì€ Falseë¡œ ì„¤ì •)\n",
        "    show_xticks : ê·¸ë˜í”„ì— xì¶• ê°’ì„ í‘œì‹œí• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ì€ Falseë¡œ ì„¤ì •)\n",
        "    '''\n",
        "    # ê·¸ë˜í”„ y ê°’ : keyì— í•´ë‹¹í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n",
        "    y = NN.history[key]\n",
        "    # ê·¸ë˜í”„ x ê°’ : epoch(1 ~ yê¸¸ì´)\n",
        "    x = np.arange(len(y))+1\n",
        "    \n",
        "    # ê·¸ë˜í”„ í¬ê¸°ë¥¼ 6x6ìœ¼ë¡œ ì„¤ì •\n",
        "    plt.figure(figsize=(6,6))\n",
        "    # ì„ ê·¸ë˜í”„ plot ('o-': ì í‘œì‹œ)\n",
        "    plt.plot(y, 'o-', color='orange')\n",
        "    \n",
        "    if show_xticks == True:\n",
        "        # xì¶• ëˆˆê¸ˆ ì„¤ì •\n",
        "        plt.xticks(np.arange(len(y)), labels=x)\n",
        "    elif show_xticks == False:\n",
        "        plt.xticks([])\n",
        "    \n",
        "    # yì¶• ë²”ìœ„ ì„¤ì •\n",
        "    ylim = [0 , np.trunc(max(y)+1)]\n",
        "    plt.ylim(ylim)\n",
        "    \n",
        "    # ì¶• ì´ë¦„, ì œëª© ì„¤ì •\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(f\"{key}\")\n",
        "    plt.title(f'History of \"{key}\" in Neural Network')\n",
        "    \n",
        "    if show_value == True:\n",
        "        # ê·¸ë˜í”„ì— ë°ì´í„° ê°’ì„ í‘œì‹œí•˜ê¸°\n",
        "        for i in range(len(x)):\n",
        "            height = y[i]\n",
        "            plt.text(x[i]-1, height + ylim[1]/100, '%.3f' % height,\n",
        "                     ha='center', va='bottom', size = 8)\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3DvfC0Be8oSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ’½ 4. ì „ì²´ ê¸°ëŠ¥ ë™ì‘í•¨ìˆ˜ ì •ì˜"
      ],
      "metadata": {
        "id": "X5S50hvn7WbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’½ ê¸°ëŠ¥ 4-1. ì „ì²´ ê¸°ëŠ¥ ë™ì‘ ê¸°ëŠ¥(ìˆœì „íŒŒ) : main_test()"
      ],
      "metadata": {
        "id": "fhGWOL8b7Yes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_test():\n",
        "    # 1-1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥\n",
        "    df = data_load(filepath=csv_dir)\n",
        "    \n",
        "    # 2-1. ë°ì´í„° ë’¤ì„ê¸° ê¸°ëŠ¥\n",
        "    df_sf = data_shuffle(df)\n",
        "    \n",
        "    # 2-2. ë°ì´í„° í‘œì¤€í™” ê¸°ëŠ¥\n",
        "    df_sc = standard_scaler(df_sf, output_dim=1)\n",
        "    \n",
        "    # 2-3. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ê¸°ëŠ¥\n",
        "    train, test = train_test_split(df_sc, train_ratio=0.8, batch_size=4)\n",
        "    \n",
        "    # 2-4. ë…ë¦½ë³€ìˆ˜ ë° ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬\n",
        "    X_train, y_train = X_y_split(train, output_dim=1)\n",
        "    X_test, y_test = X_y_split(test, output_dim=1)\n",
        "    \n",
        "    # 3. ì¸ê³µì‹ ê²½ë§(ANN) - ìˆœì „íŒŒ (ê°€ì¤‘ì¹˜ í¸í–¥ ìƒì„± ê¸°ëŠ¥ í¬í•¨)\n",
        "    NN = NeuralNetwork(input_dim=8, output_dim=1)\n",
        "    NN.feed_forward(X_train, y_train, mb_size=4, verbose=1)"
      ],
      "metadata": {
        "id": "RTkMaLN78rDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’½ ê¸°ëŠ¥ 4-2. ì „ì²´ ê¸°ëŠ¥ ë™ì‘ ê¸°ëŠ¥(í•™ìŠµ, ì‹œê°í™”, ì˜ˆì¸¡, ê²€ì¦) : main_additional_test()\n"
      ],
      "metadata": {
        "id": "Yh-ot_wr7bfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_additional_test():\n",
        "    # 1-1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥\n",
        "    df = data_load(filepath=csv_dir)\n",
        "    \n",
        "    # 2-1. ë°ì´í„° ë’¤ì„ê¸° ê¸°ëŠ¥\n",
        "    df_sf = data_shuffle(df)\n",
        "    \n",
        "    # 2-2. ë°ì´í„° í‘œì¤€í™” ê¸°ëŠ¥\n",
        "    df_sc = standard_scaler(df_sf, output_dim=1)\n",
        "    \n",
        "    # 2-3. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ê¸°ëŠ¥\n",
        "    train, test = train_test_split(df_sc, train_ratio=0.8, batch_size=4)\n",
        "    \n",
        "    # 2-4. ë…ë¦½ë³€ìˆ˜ ë° ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬\n",
        "    X_train, y_train = X_y_split(train, output_dim=1)\n",
        "    X_test, y_test = X_y_split(test, output_dim=1)\n",
        "    \n",
        "    # 3-1. ì¸ê³µì‹ ê²½ë§(ANN) - ì—­ì „íŒŒ, í•™ìŠµ ì¶”ê°€ êµ¬í˜„\n",
        "    NN = NeuralNetwork_additional(input_dim=8, output_dim=1)\n",
        "    NN.training(X_train, y_train, mb_size=4, epochs=5, learning_rate=1, verbose=2)\n",
        "    \n",
        "    # 3-2. í•™ìŠµê³¡ì„  ì‹œê°í™”\n",
        "    plot_history(NN, key='Accuracy', show_value=True, show_xticks=True)\n",
        "    \n",
        "    # 3-3. TestData ì˜ˆì¸¡\n",
        "    print(f\"[Label_data] TestData\\n{y_test}\\n\")\n",
        "    y_pred = NN.predict(X_test)\n",
        "    print(f\"[Prediction] TestData\\n{y_pred}\\n\")\n",
        "    \n",
        "    # 3-4. TestData ê²€ì¦\n",
        "    NN.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "L5MoYR6e8uUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}